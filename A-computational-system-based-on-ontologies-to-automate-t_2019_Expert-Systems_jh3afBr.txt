We have developed, in collaboration with medical and computer experts, the ontology-based Medical Report Mapping Process to support the transformation of unstructured reports into a structured representation. Nevertheless, the techniques employed in this two-phase process must be performed individually and manually by computer instructions, which hinder their use by users not familiar with such language. Thereby, this work proposes a tool to automate and optimize this process by integrating its techniques in a computational system, which was built using a software engineering prototyping approach. This system was experimentally evaluated by applying it to a set of 100 textual reports. The first phase decreased the total number of phrases (853) and words (2520) by 82.25% (48) and 92.70% (184), respectively. In the second phase, 100% of the relevant pieces of information (previously established) present in the reports were transcribed. Also, the second phase was applied, using the same configuration as the first study, in another set with 250 textual reports, resulting in a mapping rate of 99.74%. The unprocessed and unmapped words, regarding both experimental evaluations, were recorded for later inclusion into the ontology. By using this system, efficient and scalable investigations can be performed from medical reports, contributing to generate new knowledge. Also, the system facilitates the definition of these structures due to the feasibility to analyze different sentences in unique phrase sets.  1. Introduction  Currently, in the medical area, large amounts of data are daily recorded in databases in order to maintain the patients' clinical history. In general, these data are obtained from clinical examinations, in which findings are described in natural language in textual reports to summarize the observations on the state of the patient's health ( Lee et al., 2013; Oliva, 2014 ).   In these reports, it is possible to find relevant patterns that can be used to build predictive models, providing support to experts in decision-making processes ( Borrego & Quaresma, 2013; Cherman et al., 2008b; Jones, Phalakornkule, Fitzpatrick, Iyer, & Ombac, 2011; Lee et al., 2011 ). However, the increase in the volume of medical data has contributed to the emergence of large medical databases, making their manual analysis an infeasible task. It is therefore important to develop computational technologies to support the analysis and management of this information ( Han, 2011 ). Many computational methods have been developed and applied in different areas of knowledge, aiming to assist in data analysis a nd management. Among these methods, the Data Mining (DM) process, supported by Machine Learning (ML) techniques, has attracted the interest of many researchers, as it creates descriptive and predictive models from knowledge existing implicitly in the data ( Witten, Frank, & Hall, 2011 ). To this end, it is necessary to represent data in a structured format, such as the one used by Computational Databases (DB) composed of attributes and their respective values, i.e. , the attribute-value representation ( Han, 2011 ).  In the medical area, data are generated by various sources and may be represented into images, audio, video, reports, forms, and other formats. Therefore, to apply the DM process supported by ML techniques, it is required to create methods that make it possible to transform these data into a structured format, such as the one used by DB ( Lee et al., 2011; 2013 ). Also, large amount of medical data are text-based. In this sense, text mining methods can be applied in biomedical texts in order to automate the relevant patterns extraction from unstructured text ( Flucke & Hofmann-Apitius, 2014 ). However, the identification and understanding of information that was automatically extracted from texts may not be a trivial task ( Goulart, de Lima, & Xavier, 2011 ). These difficulties associated to text-processing can be caused by several factors, such as spelling and typing errors, missing information, synonyms occurrence, and several possibilities for phrase writing. Therefore, the Laboratory of Bioinformatics (LABI) from the Western Paraná State University (UNIOESTE), in partnership with the Laboratory of Computational Intelligence (LABIC) from the University of São Paulo (US) and the Department of Coloproctology from the University of Campinas (UNICAMP) has been developing a multidisciplinary project identified as Intelligent Data Analysis. This project aims to develop methods and tools that support the analysis of medical data. Part of this project consists of the development of the Medical Report Mapping Process (MRMP), which aims to transform sets of reports into a structured representation, such as attribute-value table 1 ( Lee et al., 2011 ). MRMP is based on the construction of Mapping Rules (MR), which are represented into ontologies and used for processing medical reports aiming to transfer their knowledge into structured databases ( Chute, 2005; Costa et al., 2010; Oliva, 2014 ). Although the MRMP is not restricted only to Computer Science experts, all the techniques within this method must be executed individually and manually by using computational instructions. In practice, this makes it difficult for professionals without computational expertise to interact with the method. Thus, this paper presents a Computational System (CS) that was developed to reduce the complexity for using of the ontologybased MRMP by integrating its text-processing methods. This system stands out due to some features, such as flexibility and ability to structure textual report data compatible with knowledge extraction procedures. To do so, the CS requires only the building of essential structures to make possible the report processing. Also, this system enables the use of MRMP by professionals without computational expertise, and it allows one to analyze different sentences by using unique phrase sets, which facilitates the definition of structures for MRMP application. To the best of our knowledge, the semi-automated MRMP represents a unique combination of methods for biomedical text-processing.  2. Related work  Several methods and tools for text-processing have been proposed and developed in order to assist information extraction from biomedical data ( Ruiz-Martínez, Valencia-García, Fernández-Breis, García-Sánchez, & Martínez-Béjar, 2011 ). Text mining methods, for example, can be used to automate knowledge extraction from biomedical unstructured text. In addition, these methods can be applied in systems modeling ( Flucke & Hofmann-Apitius, 2014 ). The Named-Entity Recognition (NER) is an approach often used for knowledge extraction from biomedical texts. This task has been considered to recognize and categorize entity names related to biomedical domains ( Lee, Hwang, Kim, & Rim, 2004; Nadeau & Sekine, 2007 ). NER maps elements in texts into predefined categories, e.g. , genes, diseases or drugs ( Savova et al., 2010 ). The knowledge extracted from biomedical texts can be represented and organized into ontologies. The gap between texts and the representation of inherent information is related to the extracted terms 2 and relations among these words (semantic) ( RuizMartínez, Valencia-García, Martínez-Béjar, & Hoffmann, 2012 ). In this sense, methods for information representation have been proposed in the literature, such as the Unified Medical Language System (UMLS) 3 , which is a compendium that contains various nomenclatures, controlled vocabulary, and ontologies for different domains. Approaches based on NER and UMLS was used in several biomedical applications with Natural Language Processing (NLP) and text mining. In Lee et al. (2004) , a NER method based on Support Vector Machine (SVM) is presented. This method consists of two phases: (1) Named Entity (NE) boundary 4 identification and (2) semantic classification. Phase (1) identifies the boundaries of bio-entities for one SVM classifier and conducts post-processing using a simple dictionary look-up for correction of SVM errors. Phase (2), in turn, classifies the identified bio-entities into their semantic classes through hierarchical multi-class SVMs using GENIA Ontology ( Kim, Ohta, Tateisi, & Tsujii, 2003 ). This method was evaluated using the GENIA corpus achieving an accuracy value of 74,4%. In Gong, Yuan, Wei, and Sun (2009) , a hybrid method to identify biomedical entity names was proposed. This method consists of three phases: (1) Part-of-Speech (POS) tagging; (2) noun phrase recognition; and (3) biomedical NER application. In Phase (1), POS is used to assist in the mapping of individual words to their lexical classes, for example, noun, verbs, adverbs and adjectives. POS is a set of lexical items that have similar grammatical features and can be processed using Stanford Parser ( Klein & Manning, 2003 ). In Phase (2), rules and algorithms are used in order to group words into sentences for finding the exact noun phase. In Phase (3), biomedical NER is used to find the candidate biomedical entities by matching between noun phrases and concepts from biomedical ontology, such as the Gene Ontology ( Ashburner et al., 20 0 0 ) and Disease Ontology ( Hadzic, Chang, Wongthongtham, & Meersman, 2004 ). For experimental evaluation, the purposed method was applied in the GENIA corpus achieving a precision value of 78%. The clinical Text Analysis and Knowledge Extraction System (cTAKES) presented in Savova et al. (2010) aims to extract and process electronic medical texts. This system contains some components for text-processing, such as a POS tagger and a NER annotator. Also, it extends the open-source tool OpenNLP 5 , which contains methods for processing biomedical texts ( Buyko, Wermter, Poprat, & Hahn, 2006 ). The authors applied cTAKES in a subset of clinical notes from the Mayo Clinic EMR (Electronic Medical Records) reaching an accuracy of 93.6% (POS tagger component) and 85.9% (NER annotator component). In Ruiz-Martínez et al. (2011) , a method for building biomedical ontologies by using text-processing to assist experts to extract knowledge from these documents was proposed. In particular, this method combines Knowledge Engineering and NLP methods in order to obtain relevant concepts and relations among them to be  included in the ontologies. In these structures, their isolated concept regions are connected using UMLS. This approach was applied in the cancer domain. According to the best scores, it was reached an accumulated precision of 74.79%. A NER system based on the MetaMap 6 tool ( Aronson, 2001; Aronson & Lang, 2010 ), a useful open source program for mapping biomedical text to the UMLS Metathesaurus ( Humphreys, Lindberg, Schoolman, & Barnett, 1998 ) is presented in Khordad, Mercer, and Rogan (2011) . In particular, the proposed system aims to find phenotype 7 names and their semantic types in biomedical literature by using the UMLS Metathesaurus and the Human Phenotype Ontology ( Robinson & Mundlos, 2010 ). This system was experimentally evaluated using a corpus that contains 120 sentences with 110 phenotype phrases. As a result, it obtained a precision rate of 97.6%. In Cui et al. (2014) , a system called Phenotype Exaction in Epilepsy (PEEP) is presented. Its main idea is to extract complex epilepsy phenotypes and their correlatedanatomical locations from clinical discharge summaries, a primary data source for this purpose. This system generates candidate's phenotypes and anatomical location pairs using a NER method based on the Epilepsy and Seizure Ontology ( Sahoo et al., 2014 ) from the MetaMap tool. The PEEP performance was evaluated using 400 identified discharge summaries as training data and 262 as test data, resulting in 92.4% precision. In Pletscher-Frankild, Pallejà, Tsafou, Binder, and Jensen (2015) , the DISEASES 8 system, which aims to extract disease-gene associations from biomedical abstracts using an efficient Dictionary-Based Tagger (DBT), is presented. This tagger contains names and synonyms extracted from the Disease Ontology ( Schriml et al., 2012 ) and has been used in NER regarding human genes and diseases. DISEASES is a tool freely available for download that combines the DBT with a scoring scheme, which in turn uses co-occurrences both within and between textual phrases. In addition, this system integrates results generated by text mining with manual associations among curated disease-gene, cancer mutation data and genome-wide association. In an experimental evaluation, the DISEASES was applied in the Genetics Home Reference 9 ( Mitchell, Fomous, & Fun, 2006 ) and UniProt Knowledge base (UniProtKB) 10 ( Consortium, 2014 ) databases. This approach reached 0.16% false positive rate. The authors in Malhotra et al. (2015) developed an ontology to represent Multiple Sclerosis (MS) terms. Some authors and experts built the ontology by using the Protégé11 environment. Furthermore, a dictionary with semantic synonyms and translations to different languages was developed in order to mine EMR. Moreover, the MS Ontology was integrated with other ontologies and dictionaries in a text-mining tool identified as SCAIView 12 ( Gattermayer, 2007 ). The EMR of 624 patients with MS were analyzed using the MS Ontology to identify drug usage and comorbidities regarding this pathology. This approach retrieves 80% of the genes associated with MS from the processing of textual abstracts. In Spasi ´ c, Zhao, Jones, and Button (2015) , an open-source, ontology-driven and rule-based system named KneeTex, which extracts information from textual reports that describe the Magnetic Resonance Imaging scan of the knee, is described. This system uses an ontology to guide and limit text analysis. The ontology used in the KneeTex was built through automatic term recognition (method for identifying textual representations of a particular specific domain) using MetaMap with FlexiTerm ( Spasi ´ c, Greenwood, Preece, Francis, & Elwyn, 2013 ). Also, its ontology represents terms from the training dataset that were not found in the UMLS. In the experimental evaluation, the KneeTex with Taxonomy for RehAbilitation of Knee Conditions Ontology ( Button, Van Deursen, Soldatova, & Spasi ´ c, 2013 ) was applied in a set of 100 MRI reports, resulting in 98% precision. The model proposed in Shen, Liu, Sohn, Larson, and Lee (2016) analyses predicate patterns in text related to colorectal surgery complications and generates a similarity matrix based on the relationships between the patterns. To do so, the system named BmQGen was implemented and used to evaluate the proposed model by processing colorectal surgical reports. In the experiment, 1980 reports from 1416 patients were converted to six ontologies. Each ontology contains terms related to a particular colorectal surgery complication, e.g. , infection. Thus, the Hierarchical Fuzzy C-Means clustering ( Bezdek, Ehrlich, & Full, 1984 ) was applied to a 83 ×83 predicate similarity matrix generated by processing colorectal surgical reports. As result, eight different clusters were generated, which were validated by experts. Other systems and methods based on biomedical ontologies and text-processing procedures have been proposed. Table 1 shows properties of methods applied for processing of biomedical texts, including the papers described in this section and ours. To compare these methods, the following criteria were adopted: 1. Domain application : defines input data submitted to the application of a related method; 2. Degree of automation : classifies the related methods into automated and semi-automated. In this paper, a related method is considered automated whether it is a system with Graphical User Interface (GUI); 3. Language independence : corresponds to the language dependence, i.e., the text language required to use the system. For example, most of the related publications presented in Table 1 can process only texts written in English. In this table, the methods that are language independent are marked with "yes"; 4. Unique phrases analysis : defines whether all the existing distinct phrases in text set are extracted for a specific purpose. This criterion can be useful, as these texts may contain similar phrases, which makes harder to analyze their contents to build ontologies or other representation structures; 5. Text preprocessing : determines whether a related paper applies text preprocessing methods in the input data; 6. Ontology use : checks whether a related work uses at least one ontology; 7. Ontology building : defines whether a related paper builds ontology or is able to build ontologies. If a method does not use ontologies, this criterion is disregarded; 8. Structured DB building : checks whether databases structured in an appropriate format for the data mining application are built. In comparison with related work, our proposal aims to represent unstructured textual medical reports into a structured DB, such as an attribute-value table, to enable the use of computational tools for the analysis of these data. For example, the MRMP output can be used as input for ML algorithms able to build classifiers. Also, our method eases the conduction of detailed studies on information extracted from medical reports. In particular, these studies can involve the analysis of disease incidence, development of mechanisms for illness prevention and diagnosis, among others. As related work do not share the same features of our proposal, a direct experimental comparison with them is not possible.  3. Materials and methods  3.1. Medical reports  In this work, we used two sets (RS1 and RS2) of artificial textual reports regarding upper gastrointestinal (GI) endoscopy examinations written by medical experts from the Department of Coloproctology at UNICAMP. These sets contain 100 and 250 textual reports, respectively. The first set (RS1 – with 100 documents) was used in both MRMP phases ( Section 3.2 ). The second set (RS2 – with 250 textual reports) was computed only in the second MRMP phase. The RS1 set was used in both MRMP phase for the following reasons: building of the three structures introduced in Section 3.2.1 ; evaluation of mapping method performance in the second MRMP phase; and comparison between manual and automatic mapping. RS2, the second set, was used in order to evaluate the performance of the second MRMP phase using the same structures that were build from the RS1, but considering a different set of reports (RS2). The reports used in our study consist of valid terms, written in Brazilian Portuguese similarly to the way commonly used in real reports of colonoscopy domain ( Quilici, 20 0 0 ). Several terms in these reports correspond to the Portuguese translation of English terms from the third version of the Minimal Standard Terminology (MST 3.0) ( Aabakken et al., 2009 ) for gastrointestinal endoscopy. MST was built by a partnership among the European Society of the Gastrointestinal Endoscopy (ESGE) 13 , the American Society of Gastrointestinal Endoscopy (ASGE) 14 , the World Endoscopy Organization (WEO) 15 , and a group of international collaborators. It should be emphasized that the latest MST version does not have any official translation into Portuguese.  Fig. 1. Textual report regarding observations of the esophagus found during an upper GI endoscopy examination. The artificial reports are structured with the following elements ( Oliva, 2014 ): • Header : specifies the anatomical part analyzed during the GI endoscopy procedure. It is usually the first line of the report; • Observations : phrases describing abnormalities and other features that were found. Fig. 1 exemplifies a piece of an unprocessed textual report regarding the esophagus observations found during an upper GI endoscopy examination. The first line is the header and the remaining lines are the observations. In the report example, one can note that no abnormalities were found in the esophagus during the procedure. In the observations the following medical terms are included:  • Caliber: internal diameter;  • Distensibility: expansion capacity;  • Motility: ability to perform autonomous movements;  • EGT (Esophagus-Gastric Transition): boundary between the esophagus and stomach.  Each of the 350 reports (all the reports from RS1 and RS2) was carefully created by a medical expert, so that the set is composed by textual documents with varied content, such as abnormalities written in different ways, based on what is found in real reports.  3.2. Ontology-based medical report mapping process  The ontology-based MRMP is composed of two phases ( Fig. 2 ). In the first phase, we used text-processing techniques on the set of textual reports to identify, together with domain experts, relevant patterns required in order to build an ontology, which is used in Phase 2 to map the set of reports into a DB ( Lee et al., 2013 ). These phases are presented in Sections 3.2.1 and 3.2.2 .  3.2.1. First MRMP phase  In the first MRMP phase, the following methods are performed:  • Identification of the Unique Phrases : first, we concatenated the medical reports into a single file. Afterward, we sorted the phrases in this file alphabetically and removed the duplications, keeping only one entry for each phrase. As a result, a structure named Unique Phrase Set (UPS), represented in text format, was created ( Honorato, Cherman, Lee, Monard, & Chung, 2008 ). The UPS identification is a simple approach for reducing the variability of phrases contained in the reports to prepare for the application of other text-processing techniques, facilitating the identification of patterns in their content;  • Normalization of the UPS : after defining the UPS, its phrases are normalized by replacing: (1) uppercase characters by lowercase characters and (2) accented characters by non-accented characters ( Cherman et al., 2008b ). This technique aims to reduce the variation of characters, which can be beneficial for further textprocessing. The second type of character replacement is particularly useful to normalize Portuguese text, as this language has different accents. 16 For example, the character "c" gives rise to the variations "C", "ç", and "Ç" in Portuguese;  • Removal of stopwords : we removed from the UPS terms considered irrelevant, such as prepositions and adverbs. These terms, also named stopwords ( Makrehchi & Kamel, 2008 ), constitute a list called the stoplist, which is represented in XML (eXtensible Markup Language) 17 format. Fig. 3 presents an example of stoplist, where stopwords is a tag that represents the list of irrelevant terms, number is the stopword amount, and stopword is tag that represents an irrelevant term;  • Application of lemmatization : this procedure is intended to morphologically reduce each term to its canonical form, also called the lemma ( Jongejan & Dalianis, 2009 ). This is accomplished by simplifying terms, such that, for example, conjugated verbs assume their infinitive form and plural nouns assume their singular  counterpart;  • Building of the standardization file : after defining and normalizing the UPS, we and domain experts built a Standardization File (SF), which is also represented in XML format. To do so, we defined Standardization Rules (SR), which are used to replace words and sentences by their respective synonyms, reducing the variability of the phrases inherent to these reports ( McCray, Srinivasan, & Browne, 1994 ). The standardization file is expanded with more terms during the next Phase 1 techniques ( Cherman et al., 2008b ). It should be emphasized that, although, other techniques to reduce variation of medical text have been studied in the literature ( Berndt, McCart, Finch, & Luther, 2015; Doan, Conway, Phuong, & Ohno-Machado, 2014; Karimi, Wang, Metke-Jimenez, Gaire, & Paris, 2015; SeguraBedmar, Martinez, & de Pablo-Sánchez, 2011 ), few of them have already considered the Portuguese language. Fig. 4 illustrates an example of SF, where pattern is a tag that represents the SF header, number is the amount of MRs presented in SF, synonym is a tag that characterizes an MR, n is the number of terms or/and sentences ( new tag) which must be used to replace the one represented in old tag. In this sense, an old term or sentence can be replaced by one or more new words and/or expressions. These text-processing methods were implemented, using Perl 18 language, in previous work related to MRMP ( Cherman et al., 20 08b; 20 08c; Honorato et al., 2008; Honorato, Cherman, Lee, Monard, & Wu, 2007 ). For integration of these methods in the CS, some adaptations were made in their codes. Fig. 5 presents an example of UPS generation, in which all UPSs at the right side contains a repeated phrase (into the red rounded rectangle) and the UPSs at the left side does not include repeated phrases. Thus, the UPSs without repeated sentences are used as input of text-processing techniques. Thus, as presented in this figure, the report sentences were processed according to the following steps:  1. Generation of the unprocessed UPS (UPS1);  2. Normalization of the UPS1, generating the normalized UPS (UPS2);  3. Removal of repeated sentences in the UPS2;  4. Removal of stopwords, such as "with" and "of", in the UPS2, constructing the normalized UPS without stopwords (UPS3);  5. Removal of repeated sentences in the UPS3;   6. Application of lemmatization method in the UPS3 to convert word "erosions" (plural version) into its singular format("erosion"), building the lemmatized UPS (UPS4);  7. Removal of repeated sentences in the UPS4;  8. Standardization of the UPS4, considering the SF example presented in Fig. 4 , to generate the standardized UPS (UPS5);  9. Removal of repeated sentences in the UPS5.  After building the last UPS version, its phrases are analyzed with the support of experts in order to define the attributes that will be part of the DB and their possible values. These values represent relevant information found in textual medical reports ( Lee et al., 2013 ).  MRs are defined and integrated in an ontology that maps patterns for a DB ( Oliva, 2014 ). These MRs are usually established to represent phrases with words presented in the following order:  1. Location : the vocable that specifies an anatomical part of the human body;  2. Characteristic : information about a specific location, such as abnormalities or other characteristics;  3. Subcharacteristic : supplementary information about a particular  characteristic, such as abnormalities and measures, for example.  Phrases in medical reports take into account this order, as they usually follow the format location-characteristic or location-characteristic-sub-characteristic . For example, in "distal_third_mucosa erosion_presence yes", from Fig. 5 , the distal_third_mucosa is an anatomical body part (location), the "erosion_presence " determines whether there is the erosion abnormality in the location (characteristic), which can be "yes " or "no ".  An ontology consists of a data structure used to represent knowledge in a given domain by defining a set of primitives (classes, attributes, and relations), that models concepts related to that domain ( Guarino & Giaretta, 1995 ). In an MRMP ontology, attribute descriptions and their respective MRs are represented using OWL (web ontology language 19 ) format.  Fig. 6 illustrates a general ontology that may be used to map medical reports from various domains, such as upper and lower gastrointestinal endoscopy. In this figure, rectangles correspond to classes and lines represent the hierarchical relationship among these classes. In what follows, the main classes considered in this work are described.  • Thing : the main class, which corresponds to all individuals represented by the ontology;  • Attribute : DB attributes, which are divided into two subclasses:  • Attribute_name : attribute names;  • Attribute_type : possible values for each attribute;  • Term : contains medical terms present in the phrases of textual reports. These terms represent anatomical parts, characteristics and subcharacteristics. In this context, the terms represented by instances of Term are used to build MR of the attributes. This class is divided into two subclasses:  • Region : represents terms related to anatomical parts of the human body;  • Observations : contains terms that represent information regarding a region. This class consists of two subclasses:  • Characteristic : considers terms related to potential abnormalities or other information about the region;  • Subcharacteristic : involves terms which represent supplementary description of the characteristics.  It should be emphasized that the figure shows a general ontology, such that more subclasses of Region and Observation can be included depending on the complexity of the domain.  3.2.2. Second MRMP phase  Phase 2 initially selects the set of textual reports to be processed. Some possible candidates include the set used in Phase 1 and/or another set. Afterward, text-processing techniques can be applied in the set aiming to reduce the variability of its phrases. Thereby, the sentences in each textual report are standardized in the location-characteristic or location-characteristic-subcharacteristic format, which is required for the mapping method application. It is important to note that the same text-preprocessing method is used in both MRMP phases. However, while in the first phase these techniques are computed to define auxiliary structures (stoplist, SF, and ontology) by analysis of UPSs, in the second phase, they are computed to standardize each report selected to be mapped, aiming to format its sentences to submit them to the mapping method. Although users can select different preprocessing methods for report transformation, it is recommended to apply the same that were computed to build UPSs. The reason for this recommendation is that report preprocessing directly affects the mapping method performance. Fig. 7 illustrates the preprocessing of a report fragment example. The text processing methods used to generate UPS2 to UPS5 are applied, at the same sequence ( e.g. , according to presented in Fig. 5 ), in the selected reports. Finally, the information contained in the preprocessed reports is mapped into the DB with the aid of an ontology, which is used in turn as a query tool by the medical reports mapping method in order to find an MR for each textual sentence. During the mapping method application, each phrase is processed, through pattern matching, using an ontology, in which a compatible MR is searched. The mapping method is computed into tree steps: 1. Report split by sentence: each textual report is divided into a phrase set; 2. Sentence processing: each word from sentences are processed and associated, by matching operation, to an ontology class that composes an MR. If the processed sentence matches with an MR, the attribute related to this rule is filled in the record corresponding to the processed document. In this sense, for each textual report, a record, composed by a value for each attribute registered in the ontology, is generated. Each record corresponds to a line of the DB, which is represented as an attribute-value table in CSV (Comma-Separated Values) format. On the other hand, the unidentified terms are included in the Unprocessed Term List (UTL), which is stored in a text format file; 3. Unmapped attributes filling: after the processing of a report, the attributes that were not detected in such document, by pattern matching between sentences and MRs, are filled with the hyphen character ("-") in their respective field in the report record. Fig. 8 exemplifies the mapping application in a preprocessed report ( Fig. 7 ). In this scheme, an ontology and a standardized report are used as input in the mapping method, which tries to match each sentence with an MR. The phrase "distal_tird_mucosa erosion_presence yes", for example, matches with an MR of the lower_esophagus_erosion attribute, whose column in the attribute value table is filled, by "yes" value, in the line related to the mapped report. For the first sentence in the report, word "esophagus" was added in the UTL because its sentence does not complete an MR. It is important to emphasize that the mapping method was implemented in Java 20 under the aid of OWLAPI 21 , which is an interface and implementation for the OWL language. This API (Application Programming Interface) includes an OWL parser that is used in the MRMP search for associations between mapper rules and attributes that correspond to domain relevant content inherent to textual phrases under processing.  3.3. Software development process: Prototyping  To integrate the techniques used in MRMP, a computational system was built using a software engineering approach called prototyping ( Pressman, 2011 ). This method is suitable for problems with complex requirements and availability of experts during system building. Prototyping is employed according to five steps: (1) communication; (2) quick plan; (3) modeling and quick design; (4) prototype construction; (5) deployment, delivery, and feedback ( Pressman, 2011 ). In Step (1), meetings were held with domain experts in order to identify the system requirements. Initially, two main requirements were defined: (1) to achieve an automatic or semi-automatic report mapping process and (2) to develop a user-friendly and intuitive graphical user interface to facilitate the system use. In Step (2), the requirements were detailed and analyzed to verify the feasibility of developing a CS. Afterward, the technologies that could be used to accomplish the requirements were chosen, such as: Java, JRuby 22 , Ruby on Rails (RoR) 23 , HTML (HyperText Markup Language) 24 , SQLite 25 , CSS (Cascading Style Sheets) 26 , JavaScript. 27 In Step (3), the CS was modeled to better understand the requirements. Thus, the system was structured into two scenarios, one for each MRMP phase. In particular, the first phase scenario consists of the following features: • Management of the medical reports set : makes it possible to select and open sets of textual reports in a file list. In addition, this feature allows the system to display the content of each report; • Building and management of the unique phrase set : simplifies the application of text-processing techniques on the set of reports selected to generate the UPS, as well as implements the results visualization and recording procedures; • Definition and management of stopword list : this feature allows the user to build a list of irrelevant terms, which is shown on a stoplist display panel. Likewise, the user can save the stoplist and remove stopwords; • Construction and management of standardization file : supports the definition of a list of standards to be applied for medical report processing. These standards are grouped into two sets:     one set consists of a list of the original terms to be replaced and the other one corresponds to the list of substitute terms. This feature also makes it possible to load and save the SF, as well as to use it to standardize the UPS;  • Definition and management of attribute set : the user can specify MRs and attributes for an ontology, as well as to load and save ontologies.  In what follows, the second scenario features are defined:  • Preprocessing conduction : allows the system to select and apply text-processing techniques on the reports set defined for this purpose. In particular, this set could be composed of the documents selected in Phase 1 or another set of reports. Moreover, the preprocessed reports are saved locally on the user's computer;  • Mapping conduction : allows the system to fill in the DB by mapping the preprocessed reports with the aid of an ontology. Afterward, the results (DB and UTL) are saved locally on the user's computer.  The MRMP conduction results, such as the filled DB, the UPS, and the processed reports, are stored on the user's computer. In Step (4), based on the requirements from the previous steps, the system was implemented using the JRuby programming language with the RoR framework. As mentioned, JRuby allows the integration between Java and Ruby tools. Also, JRuby supports the RoR framework adoption in order to simplify the development of a web system that enabled for remote use, avoiding configuring the user's computer. Furthermore, RoR is based on the MVC (Model-View-Controller) architecture, which is widely employed for web systems ( Ruby, Thomas, & Hansson, 2013 ). To assist the MRMP integration with the CS, the following methods were developed in Java:  • UPS manager : performs MRMP text-processing methods to build UPSs;  • Stopword manager : builds and loads the stoplist;  • SR manager : builds and loads the SF through the user-defined SRs;  • Parameter configuration manager : defines the techniques to be applied to preprocess the reports in the second MRMP phase;  • Report preprocessor : applies the text-processing methods specified by the user in the parameter configuration manager to preprocess the set of reports.  To simplify the integration of the MRMP using ontologies with the system, the following aspects were taken into account:  • The support features to allow users to select ontologies;  • An attribute manager to allow users to generate ontologies by defining MR;  • The requirement that all MRs should be specified according to the location-characteristic and/or location-characteristic-subcharacteristic format;  • Loading and storing the attribute-value table and the UTL.   The system interface was designed using the HTML language to build web pages, in which the features offered by the system are displayed. Furthermore, the CSS language was considered to define the layout of the web pages. JavaScript, in turn, was used to control some interface features. SQLite was used only to control the user authentication .  In Step (5), the system was evaluated by domain experts in order to define which requirements might be modified, removed or included.  3.4. Experimental evaluation of the system  After building the system, it was evaluated in an experimental study regarding the performance of the semi-automated MRMP, considering computational cost, time, and efficiency to map attributes contained in all reports mapped into the DB. This study included two sets of artificial medical reports. The first set (RS1) was used to build structures (first MRMP phase) and mapping application (second MRMP phase). Altogether, the 100 documents in the RS1 set contain 400 phrases and 2520 words. Each RS1 textual document contains, on average, four phrases. The second set (RS2) was processed, aided by structures built from RS1, only for application of second MRMP phase. It is important to highlight that the amounts of phases and words were measured only by RS1 because these values were used in the evaluation of Phase 1. On the other hand, as the RS2 was only used in Phase 2, it was not needed to compute the amount of phrases and words for this set. In the first MRMP phase, all the phrases in the report set RS1 (with 100 documents) were concatenated and sorted alphabetically into a single file. Subsequently, all duplicated phrases were eliminated, keeping only one instance of each phrase to create the UPS1 (unprocessed UPS). Afterward, in the UPS1, all uppercase characters were replaced with lowercase characters, whereas accented characters were replaced with the unaccented counterparts, yielding the UPS2 (normalized UPS). We then analyzed the UPS2 and defined a list of 59 stopwords. This stoplist was applied to the UPS2 through the stopword removal method, yielding the UPS3 (normalized UPS without stopwords). In turn, the recently generated UPS was processed though the lemmatization method in order to create the UPS4 (lemmatized UPS). Standardization Rules (SR) were then prepared together with domain experts by analyzing the previously built UPSs (UPS1, UPS2, UPS3, and UPS4). As a result, we constructed an SF composed of 81 SRs that converted the original phrases to the location-characteristic or location-characteristic-subcharacteristic format, generating the UPS5 (standardized UPS) and simplifying the MR definition. After creating all UPSs, their respective contents were analyzed in order to define the attributes and values that would constitute the attribute-value table (database) and the ontology structure, which consists of 16 categorical attributes and 35 MRs. For the sake of completeness, all attributes included in the ontology used in this work are described in the Supplementary Material. Table 2 describes three examples of attributes included in the ontology. It should be emphasized that all of these attributes are named in Portuguese, which is the language of the reports evaluated by us. 28 In the second MRMP phase, the medical reports of the RS1, described in Section 3.1 , were submitted to text-processing techniques in order to make them suitable for the mapping method. Thus, the documents were preprocessed through the sequential application of the following techniques: normalization, stopword removal, stemming, and standardization. Afterward, the information present in the preprocessed reports was mapped to the database. This mapping was performed in two ways: manually and automatically. In particular, an individual conducted the manual mapping with a spreadsheet program, such as Microsoft Excel. On the other hand, the system developed in the present work performed the automatic mapping. Subsequently, the stoplist, ST and ontology generated from RS1 were used to process the 250 textual reports from RS2 by applying the second MRMP phase.  Finally, the results obtained by applying the semi-automated ontology-based MRMP were analyzed according to the following criteria:  • Statistical measures related to UPS creation (only for report set RS1);  • Computational cost to build the UPS;  • Computational cost and time to implement the first MRMP phase;  • Percentage of attributes contained in all reports mapped to the database (for both report sets);  • Incidence of unmapped terms (for both report sets);  • Time cost for manual mapping;  • Computational cost for automatic mapping (only Phase 2);  • Computational cost to apply the MRMP (Phase 1 and 2) by using the CS.  4. Results and discussion  The methods implemented in the CS have been evaluated in previous studies on artificial textual reports of upper GI endoscopy examinations ( Costa et al., 2010; Honorato et al., 2008; Lee et al., 2013; Oliva et al., 2014 ) and showed good results, agreeing with the results obtained in experiments using artificial data. Also, these methods were successful in previous work related to reports of colonoscopy examinations ( Cherman et al., 20 08b; 20 08c; Honorato et al., 2007 ). Despite of the MRMP support, the use of this process by professionals without computational expertise may not be a trivial task. As mentioned before, the MRMP must be executed by using computational instructions. This may even involve direct changes to the source code. Moreover, to build the SF and stoplist, the user has to manually represent their content in XML structures according to a standardized and hierarchical schema. The manual organization of that information into XML structures is a repetitive and costly task, which may lead to representation errors and, consequently to influence the results of the textual report processing. To reduce these difficulties, we developed a CS in order to integrate all the techniques used in MRMP. This system simplifies the use of MRMP, for example, by avoiding asking the users to specify the technical details (e.g., knowledge about markup languages) of the methods used in the process. To do so, each system scenario provides resources to ease and (semi-)automate report processing.  4.1. Computational system  Figs. 9 and 10 show the main screens of the first CS scenario regarding the "Standardization File Building and Management" and "Attribute Set Definition and Management" features. In these figures, all examples were written in Brazilian Portuguese. Each screen takes into account examples of artificial textual reports. In what follows, each screen and the corresponding feature are discussed. The "Standardization File Construction and Management" feature, related to the screen shown in Fig. 9 , speeds up the tasks of defining SRs and exporting them to a SF. This figure shows a UPS text area (left side) and a list of the original terms and a list of substitute terms, respectively (right side). Fig. 9 also illustrates, within a smaller rectangle, a specific screen useful to define a new SR. In particular, the small screen is opened by clicking on the "New" button. By using the feature shown in Fig. 9 , the user can also apply the standardization method, save and upload SFs. The screen illustrated in Fig. 10 enables the user to develop MRs and database attributes in a quick and intuitive way. It is thereby unnecessary to use external tools to build ontologies, such as Protégé29 , because the system can generate these structures automatically from the attributes and MRs defined by users. The user can also save and load ontologies to the user's computer by using this screen, making it possible to reuse this structure in further experiments involving the MRMP. This figure shows a UPS text area at the left side followed by the attribute list, while the one at the right side refers to the fields that show the information of the selected attribute. Fig. 10 also presents, within an external rectangle, a small screen that is opened by clicking on the "New" button. The new graphic user interface is particularly useful to define a new attribute and its respective MRs. By using this small screen, the user does not need to use ontology frameworks and to know OWL language to generate ontologies compatible with the MRMP process. The first CS scenario enables the user to upload textual reports remotely from his computer and to visualize the corresponding content in the system, as well as to build and view UPSs through the automatic application of text-processing methods in these reports. Also, our system simplifies the definition of stopwords, SRs, MRs, and attributes, which will compose the stoplist, the standardization file, the ontology, and the database, respectively. Although the construction of these structures is carried out by humans and requires some time at first, they can be reused in new applications within the same domain. Occasionally, small changes in the structures might be needed to deal with new terms found in the new sets. The remaining screens related to first CS scenario are presented in the Supplementary Material. Fig. 11 shows the main screen of the second scenario regarding the "Mapping Application" feature. In the "Mapping Application" feature, users can map the reports using ontologies to fill in the database. The mapping is performed by clicking on the "Map Reports" button. The screen of this feature also makes it possible to visualize the results, such as the filled attribute-value table (left side in the figure) and the UTL (right side in the figure), which are stored in CSV and text format, respectively, in a user folder at the local disk. In the table presented in Fig. 11 , each line corresponds to a report, and each column (except the report ID, i.e. , the first one at the left side) is equivalent to an attribute. The second scenario of the semi-automated MRMP makes it possible:  • Reuse of reports considered in the first scenario and/or the upload  of other sets of reports;  • Selection and application of text-processing techniques, such  as normalization, stopwords removal, lemmatization, and standardization;  • Mapping of standardized reports using ontologies to fill in the  database;  • Visualization of the mapping method results, such as the filled attribute-value (database) table, and the UTL.  In addition, the "Preprocessing Application" (Supplementary Material) and "Mapping Application" ( Fig. 11 ) features reduce the human cost to process medical reports. In the "Preprocessing Application" feature, the expert needs only to load the textual reports, as well as the structures (stoplist, standardization file, and, ontology) corresponding to the initial knowledge representation in the considered domain. Thus, the remaining processing (report preprocessing and mapping to a structured database) is automatic. The attribute-value table generated by the CS can be submitted for the application of intelligent processes for knowledge discovery. Also, the UTL can be used to update all structures used by the MRMP, such as the stoplist, standardization file, and ontology. In this context, it is important to note that the CS can be used remotely, via the Internet, enabling its use without installing and setting up a computing environment. This feature makes the system widely available for use, as only an Internet access is required. Also, unlike the previous implementation, the proposed system offers the users a high level of access to techniques implemented in the MRMP, simplifying the use of the process. In addition, the CS was developed with free software tools. It should be emphasized that the system is accessed only by previously registered authorized users.  4.2. Experimental evaluation  The MRMP semi-automated by the CS presented in this work was evaluated using two sets of artificial textual reports presented in Section 3.1 . In order to compare the complexity of the manual process of mapping reports to a structured database with the semiautomated one, we considered a smaller set (RS1).  4.2.1. Report processing  In the first scenario of the CS, the 100 reports from RS1 were submitted to text-processing techniques to build the UPSs, which were then used to define the stoplist, the SF, and the ontology. The content of the stoplist and SF can be found in the appendix of Oliva (2014) and the ontology can be accessed by requesting the authors. The UPSs were constructed regarding the following order: the first version of the UPS (UPS1); the normalized UPS (UPS2); the normalized UPS without stopwords (UPS3); the lemmatized UPS (UPS4); and the standardized UPS (UPS5). Posteriorly, the UPS5 was used to develop MRs and database attributes, which were integrated in an ontology. Figs. 12 and 13 show, respectively, the number of phrases and words within each UPS built in this work. The lower the amount of phrases, the more concise the UPS is. Each figure also shows the percentage of reduction achieved by each phrase set in comparison with the previous set. As Fig. 12 shows, after building UPS5, the total number of words was decreased by 82.25% (48 phrases) in comparison with the initial amount (853 phrases). According to Fig. 13 , it is observed that, by applying the preprocessing methods, the total number of phrases was decreased by 92.70% (184 words) in comparison with the initial amount (2520 words). In this sense, the UPS5 has a number of phrases and words (on average 3.83 words per phrase) considerably lower compared to UPS1, in which each phase contains on average 12.01 words. Consequently, this reduction allows the generation of more compact and representative ontologies. The decrease in the number of phrases can reduce the complexity of the reports content analysis by medical experts. Also, the reduced amount of phrases and terms makes it possible to represent the knowledge contained in medical reports in an agile and standardized form into an ontology, which is used in the second phase of the MRMP (second scenario of the CS). In the second scenario, the reports were preprocessed using the normalization, stopword removal, lemmatization and standardization methods. To do so, the stoplist and SF built in the first scenario were used. Afterward, the preprocessed documents were mapped into the database using the ontology defined previously. As a result, 100% of the relevant pieces of information (attributes established in the preceding scenario) present in the reports was transcribed. Although all the established attributes present in the reports have been mapped, 40.00% of these reports had at least one word (total amount of 85 words) that was not processed during the mapping because SRs and MRs were not defined to process them. These words can also be included in the stoplist. Accordingly, these words were include in the UTL, in which 37 different words (20.11% of the amount of words in the UPS5) were included. Table 3 shows a summarized UTL, in which the seven unprocessed terms with the higher incidence in the reports is presented. The complete UTL is presented in the Supplementary Material. In Table 3 , we can see that the unmapped words with highest incidence in the standardized reports are "nao_confluente " (not confluent), "espessado " (thicked), and "linear " (linear), which were found ten, eight and six times, respectively. It should be emphasized that the UTL can be useful as a source to define new SRs, MRs, stopwords, and attributes in Phase 1, increasing the ways for knowledge representation. In order to expand the experimental evaluation in our work, we used the stoplist, SF, and ontology previously built from the 100 RS1 reports to apply the second MRMP phase in a different set of 250 reports (RS2) regarding upper GI endoscopy examinations with esophagus observations. As a result, 99.74% of the relevant content of these documents was mapped to a structured database (attribute-value table). Another important outcome was that 16.8% of the reports had at least one term (total of 101 words) transcribed to the UTL (also presented in the Supplementary Material). In fact, the mapping performance lower than 100% is not a negative result as the UTL utility and the flexibility of the MRMP may prove their value by enabling the detection of terms that can be added to the ontology or corrections that should be made. It is the case in this work: some mistyping errors in the new set of reports, not present in the first phase of the process, were found. Thus, in this study and future applications of the MRMP, the mapping performance can be improved by the analysis of the UTL and the definition of new stopwords and SRs, as well as the inclusion of new terms into the ontology.  4.2.2. Computational cost for MRMP application  The semi-automated MRMP was also evaluated in relation to the computational cost to execute the text-processing techniques and the time estimated to build structures such as the stoplist, the SF and the ontology. For each text-processing technique employed in the MRMP, the corresponding computational cost is directly related to the number of processed reports. Table 4 presents the computational cost measured in terms of complexity for each text-processing technique. In particular, the complexity is expressed in terms of some instructions, such as the number of comparison operations performed. In what follows, the variables and respective values that were used to measure the complexity of each text-processing technique are shown:  • N R : number of reports (100);  • N AC : average number of characters per phrase (39.50);  • N AP : average number of phrases per report (4);  • N AT : average number of terms per phrase (6.30);  • N LR : number of lemmatization rules (26);  • N S : number of stopwords (59);  • N ST : number of SRs (81);  • N MR : number of MRs (35).  It is important to emphasize that the 35 MRs are organized into 16 attributes, which were not considered in the estimation of the computational cost. To convert the number of comparisons to the running time, we considered in the present study a hypothetic computer with a 10 0 0 KHz processor. This machine, able to perform a million instructions per second, is included as a reference. Accordingly, the total computational cost to implement the first MRMP phase (system scenario 1) is 24621.84 s, i.e., 6 h 50 min, and 21.84 s. This value was estimated from Table 4 as follows:  • Construction of the normalized UPS, with removal of stopwords, lemmatized, and standardized: E1 + E2 + E3 + E4 + E5 , i.e., 6,768,820 comparisons or 6.77 s;  • Task 1 : to build a stoplist with 59 terms considered irrelevant –30 min of time cost;  • Task 2 : to define a SF with 81 SRs – 3 h, 42 min, and 45 s of time cost;  • Task 3 : to develop an ontology containing 35 MRs – 2 h, 37 min, and 30 s of time cost.  In this work, the time cost for each task was estimated based on the average time spent by an expert to generate a part of stopwords, SRs, and MRs. In other words, the time approximation was made considering some report sentences, from which an average time needed to deal with each sentence in this group was used to estimate the total time spent. For each stopword, for example, the time between its identification in a UPS and its inclusion into the stoplist was, on average, approximately 30.5 s. Since 59 stopwords were defined, it took, around, 30 min or 1800 s (30.5 ∗ 59) to generate a stoplist. It is essential to note that the time estimated in this work is optimistic because the trend is that as the number of reports increases, factors, such as the expert's fatigue, would lead to a longer time than estimated. The computational cost to implement the second phase (system scenario 2) is E2 + E3 + E4 + E5 + E6 , i.e., 698,900 comparisons (0.70 seconds). Thus, the total cost of applying the text-processing techniques in both phases of MRMP is given by summing the time spent by each task and each method previously mentioned (Task 1 + Task 2 + Task 3) + E1 + 2 ∗ (E2 + E3 + E4 + E5) + E6 , i.e., 24,621.77 seconds (6 h, 50 min, and 22.54 s). For comparison purposes, the manual mapping was also performed in the reports set. To this end, an expert transcribed the information contained in these documents to an Excel spreadsheet. This spreadsheet consists of 17 columns, in which: one column identifies the reports and the 16 remaining ones represent the attributes. Then, the information from each document was manually transcribed into this spreadsheet, filling in each attribute identified in the reports. The average time for the mapping was 95 s for each report, whereas the total mapping time was of 9500 s (2 h and 38 min). To measure the total time, it is essential to take into account the time required for the preparation of the attributes that make up the database. This procedure also requires the construction and analysis of the UPS such that one can identify the patterns to be mapped. In the present work, we measured for this procedure the time cost of 10,560 s (2 h and 56 min), divided as follows:  • 5 s to copy and paste the phrases from each report into a spreadsheet program, such as Excel (total of 50 0 s – 5 s × 10 0 reports);  • 10 s to select the spreadsheet option to sort the phrases;  • 600 s (1.5 s × 400 phrases) to identify and remove duplicated phrases;  • 9450 s (similar to Task 3) for UPS analysis and database building.  Accordingly, the total time cost for the manual mapping of a set of 100 reports was 2 h and 56 min. Thereby, the estimated total time to map 800 reports was 13 h and 20 min. The time cost values for more than 100 reports were estimated based on the average time spent by the expert to process each report in the mentioned set. Besides manual mapping, the time estimation approach based on real assessment and estimate was also applied to automatic mapping.  Fig. 14 shows the relationship between the number of reports (in hundreds – axis x ) and time (in minutes – axis y ), in order to provide a more concise view of the comparative performance between manual and automatic mapping. Based on Fig. 14 , it is worth noting that, despite of the high amount of time needed to build the SF structures, ontology, database, and stoplist, the definition of these structures is performed, in the first process phase, only upon the first application of the semi-automated MRMP. Afterward, they can be reused for subsequent mapping of other reports from the same domain. Furthermore, the second process phase has a very low execution cost and each report is processed in milliseconds. In this work, the time cost to implement the second phase for each report was 7 ms given by the total time cost to implement the second phase (0.70 s) divided by the number of reports used (100). However, the time to perform manual mapping is linear, i.e., it linearly increases as the number of reports to be processed rises. Also, it is important to emphasize that the time cost of the manual performance was underestimated in this case study because external factors that could affect its performance were not considered, e.g., the time for error correction during the spreadsheet filling in and the user's fatigue when performing the filling in as the amount of reports increases. Also, it is important to note that today computers are much faster than the computer used as reference in this evaluation. Another advantage of the semi-automated MRMP is the ease to identify and include new attributes, mapper rules, standardization rules, and terms considered irrelevant. In particular, this feature is supported by the automatic identification of terms not mapped previously. It should be emphasized that, as Fig. 14 shows, the time cost to apply the semi-automated MRMP would be lower compared with the manual transcription from 260 reports with properties similar to those used in this experimental evaluation. In this context, the manual spreadsheet filling in is an unfeasible task for large amount of medical reports when we take into account the time spent and greater possibility of errors (due to human factors, such as fatigue and inattention) to perform this task. In fact, the MRMP semi-automated by CS requires less time to conduct the mapping from a certain amount of reports. Although a high time to build the stoplist, SF, ontology, and database may be needed in the first CS scenario (first MRMP phase), these structures are defined only during the first run of the semi-automated MRMP and they can be reused for further processing of other reports from the same domain. The second CS scenario, in turn, can be performed in a few seconds, i.e., its features demand extremely low running cost. In comparison with alternative proposals from the literature ( Section 2 ), our system can be used to process reports written in other languages. In this sense, for efficient system use, the unique change needed is the building of system structures in the desired language. Also, our proposal differentiates from other systems because it can generate different kinds of UPSs, enabling the comparison among them. As a result, it facilitates to build more representative structures for the MRMP application. In other words, UPSs are essential for the definition of consistent and non-redundant structures in MRMP.  4.3. Discussion on previous studies  The MRMP method was applied in previous studies by our research group and all of them reached interesting results. Table 5 summarizes some previous studies conducted. In Cherman et al. (2008a) , the first version of MRMP (MRMP1) 30 ( Honorato et al., 2007 ) was applied in colonoscopy and anorectal manometry report sets, which contains 100 and 68 textual documents, respectively. A total of 293 attributes were defined (277 and 16 for colonoscopy and anorectal manometry, respectively). In an experimental evaluation, 82% of colonoscopy report content was mapped to a structured database. For anorectal manometry reports, 100% of their content was transcribed to the database. In an other study ( Honorato et al., 2008 ), the MRMP1 was computed to process 609 GI endoscopy reports, respectively, for which, a total of 168 attributes were defined. As a result, 100% of relevant textual content was mapped to the database. It is important to emphasize that the GI endoscopy reports considered in this experiment contain observations about esophagus, stomach, and duodenum. Costa et al. (2010) used the MRMP based on an ontology (used in the next references and our current work) in order to process a GI endoscopy report set of 3647 textual documents with observations about the esophagus. In the experimental evaluation, an ontology with 15 attributes was defined and applied resulting in a rate of 85,824% relevant content mapped into the database. In the experimental evaluation conducted in Lee et al. (2013) , two report sets, one with ten GI endoscopy documents (only esophagus) and other with 100 colonoscopy reports. In this sense, ontologies with 15 and 283 attributes were defined to represent the relevant content of GI endoscopy and colonoscopy reports, respectively. As a result, 100% and 82% of the relevant information regarding GI endoscopy and colonoscopy documents, respectively, was transcribed. In this work, we conducted the experimental evaluations in two sets of GI endoscopy reports regarding only esophagus observations. The first set (RS1), which contains 100 textual documents, was used to develop an ontology with 16 attributes, whose MRs resulted in 100% of the relevant content transcribed to an attribute value table format, considering the same set of reports. Afterward, the previously developed stoplist, SF, and ontology were used to process the 250 reports of the second set (RS2). This document set was processed only in the second MRMP phase, simulating an application of the method to new reports from the same domain. As a result, a mapping rate of 99.74% was achieved. In comparison to previous findings, the conducted experimental study in this work was more detailed, since we also considered the number of words and phrases for each UPS; the number of stopwords, SRs, and MRs generated; the UTL analysis; and the time cost for MRMP application. Also, in this work, structures used in Lee et al. (2013) and Costa et al. (2010) , such as stoplist, SF, and ontology were improved by their restructuring and inclusion of stopwords, SRs, and MRs, respectively. Finally, it was possible to observe that our experimental evaluations led to competitive results in comparison to the previous findings.  4.4. Possible MRMP applications  The MRMP flexibility makes it a useful alternative for different applications. Some of them are reported in what follows:  • Our method can find relevant terms in unstructured textual reports from any medical domain and represent them in a structured format compatible with manual or semi-automatic processes for knowledge discovery and information retrieval.  • MRMP can reduce the variability inherent to a set of unstructured reports written by different medical experts by transforming variations of a phrase into a single phrase.  • Our mapping process can support the building of new domain ontologies by yielding a unique phrasis set with relevant terms.  • The previous benefits can be extended to areas unrelated to Medicine, such as Law and Accountability, in which unstructured digital text susceptible to variability needs to be retrieved and analyzed.  5. Conclusion  MRMP is a tool that can be widely used to transform sets of textual reports, written in a computationally tractable language, into a structured representation compatible with the application of the Data Mining process or the conduction of biostatistics studies. In this sense, the system can process reports written in other languages, requiring only the generation of structures, for such language, required for use of the MRMP. To make the mapping process usable for professionals from different domains, the present paper described the development of a CS able to semi-automate and enhance the MRMP by integrating the steps and techniques used therein and including ontology as a tool to represent domain knowledge. Moreover, the CS can be used for professionals without computational expertise, as its features are accessed by means of graphic and intuitive interfaces. Even though in this work, considering 100 reports (first experimental evaluation), the automatic mapping using the system took a longer time to process the reports in the experimental evaluation, the computational complexity analysis shows that for more than 260 reports, the semi-automated MRMP would perform much faster than the manual mapping. This is due to the building of specific structures (stoplist, standardization file, ontology, and database) in the first CS scenario. What makes the CS much faster is that these structures need to be built only in the first execution of the semi-automated MRMP in a specific domain. Afterward, they can be used to process other reports from the same domain. Furthermore, the second CS scenario has a very low execution cost, i.e, the second CS scenario runs in a few seconds or milliseconds. In fact, according to domain experts, this system is a promising tool to extract and study patterns that can be found in textual reports.  5.1. Main contributions  The CS developed in this work yields the following contributions:  • Automatic mapping of the reports (second MRMP phase) content in any language into a structured representation table, compatible with artificial intelligence approaches for knowledge extraction;  • Innovative combination of natural language processing methods in comparison to related work (to the best of our knowledge);  • Experimental evaluation in medical reports written in Brazilian Portuguese;  • Reducing the dependence of computer experts for using the MRMP by providing a graphic and intuitive interface;  • Tools that facilitate the definition of standardization and mapper rules without the need for technical knowledge of markup languages;  • Possibility to build ontologies in the system and to use other data structures built by the Protégé free software tool;  • Viability to reuse the knowledge representation structures for further report mapping;  • Feasibility of establishing, by standardization files and ontologies, a standard vocabulary of medical terms in a specific domain;  • The support to build large and standardized databases with information mapped from reports for further investigation;  • Possibility to build ontologies of other biomedical domains;  • Feasibility to expand the ontology developed in this work by integrating more information (mapper rules and/or attributes) about upper gastrointestinal endoscopy examinations.  5.2. Future work  Future work includes: to perform broader and more sophisticated system evaluations; to integrate other text-processing techniques in the medical reports mapping method; to conduct case studies on sets of real medical reports; and to conduct numerical comparison with biomedical text-processing methods evaluated in other languages.  